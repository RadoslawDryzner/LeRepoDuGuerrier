{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As always, we import everything\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import hdf5_getters as getters\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "from textblob import Word\n",
    "import pycountry\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from gensim import corpora, models\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project consists of exploring the lyrics of many songs and finding themes and the usage of the words used in these songs over time. We use the Million Song dataset to find information about the song as well as various other datasets and sources to find lyrics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by getting a list of all the files from our dataset. The Million Song dataset organises the dataset in multiple files and directories. The following code snippet gets all these files and prints the number of the files found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(\"million-song/data\"):\n",
    "    all_files.extend([dirpath + \"/\" + filename for filename in filenames if filename.endswith(\".h5\")])\n",
    "all_files_num = len(all_files)\n",
    "all_files_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Million Song dataset is not given in simple text but encoded using the [Hierarchical Data Format](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). The following functions are used to get the relevant data from a file. Each file is a single record of the dataset, a single song described with multiple fields. These functions simply call the getter functions provided with the dataset to access the data. Since we only need a few fields, we simply take the track id, title, artist name and year. This fields will be relevant later on for our analysis and vizualisation.\n",
    "\n",
    "The track id will obviously identify the track in our analysis while the title and the artist will help us find the lyrics of the song. The year will be used for our vizualisation tool to see the evolution of the vocabulary and themes used in the songs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_songs(filename):\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    track_id = getters.get_num_songs(h5)\n",
    "    h5.close()\n",
    "    return track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_id(filename):\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    track_id = getters.get_track_id(h5)\n",
    "    h5.close()\n",
    "    return track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(filename):\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    title = getters.get_title(h5).decode()\n",
    "    h5.close()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artist_name(filename):\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    artist_name = getters.get_artist_name(h5).decode()\n",
    "    h5.close()\n",
    "    return artist_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(filename):\n",
    "    h5 = getters.open_h5_file_read(filename)\n",
    "    year = getters.get_year(h5)\n",
    "    h5.close()\n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Million Song dataset does not contain information about the genre of the songs. However, there is an additional dataset from the same source that contains this information. Unfortunetely, it's not present for all the tracks of the Million Song dataset. We read this genre dataset here and will later link the genres with the data we obtain from the main dataset.\n",
    "\n",
    "Note that the file read here is not the one directly obtained from the source but the one where we only take the genre and the track id, since these are the only ones we need.\n",
    "\n",
    "The genre dataset only has around 60000 tracks which is substantially smaller than the Million Song in its entirety. However we believe this amount of tracks will be enough for our data analysis and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('MillionSongSubset/msd_genre_dataset.txt')\n",
    "#genre_dataset = df[['genre', 'track_id']].set_index('track_id')\n",
    "#genre_dataset.to_csv('MillionSongSubset/genre_dataset.txt')\n",
    "genre_dataset = pd.read_csv('MillionSongSubset/genre_dataset.txt').set_index('track_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function links gets the genres for a single track given its id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_song_genres(track_id):\n",
    "    if track_id in genre_dataset.index:\n",
    "        return \"&\".join(genre_dataset.loc[[track_id]].values[0][0].split(' and '))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting our data collection, we make sure that all files correspond to only one song as they should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(all_files):\n",
    "    assert get_num_songs(filename) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet reads the Million Song dataset in its entirety and uses the genre dataset to link the two. It gets all the fields we need as we discussed above and also gets the genres of each track. This information is then put into a dataframe. \n",
    "\n",
    "For convenience, we save this data in a new `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data = pd.DataFrame([])\n",
    "\n",
    "i = 0\n",
    "curr_percent = -1\n",
    "for filename in tqdm(all_files):\n",
    "    percent = int(100 * i / float(all_files_num))\n",
    "    if percent != curr_percent:\n",
    "        curr_percent = percent\n",
    "    \n",
    "    track_id = get_track_id(filename).decode()\n",
    "    genres = get_song_genres(track_id)\n",
    "    if genres:\n",
    "        to_add = [('track_id', track_id), ('genres', genres), ('artist_name', get_artist_name(filename)), ('title', get_title(filename)), ('year', get_year(filename)), ('lyrics', \"\")]\n",
    "        data = data.append(pd.DataFrame(OrderedDict(to_add), index=[0]))\n",
    "    i += 1\n",
    "\n",
    "data.set_index('track_id', inplace=True)\n",
    "data.to_csv('data/data.csv')\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('data/data.csv').set_index('track_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to obtain lyrics data for our tracks. For this, we have found two datasets. Both of these contain artist, track title and lyrics data which we read in the following code snippets. We try to get the lyrics from both datasets, but it's possible that neither of them contains the lyrics for some on our tracks. For this reason, we will also look at genius.com which is a website containing many lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df1 = pd.read_csv('lyrics/songdata1.csv')\n",
    "lyrics_df1.set_index(['artist', 'song'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df2_raw = pd.read_csv('lyrics/songdata2.csv', na_filter=False)\n",
    "lyrics_df2 = lyrics_df2_raw[['song', 'artist', 'lyrics']].set_index(['artist', 'song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_csv1(artist_name, title):\n",
    "    if (artist_name, title) in lyrics_df1.index:\n",
    "        return lyrics_df1.loc[artist_name, title].values[0][1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics_csv2(artist_name, title):\n",
    "    # In this file, the artist and title fields have hyphens instead of spaces\n",
    "    # and are exclusively in lower case, so we change our data to match this\n",
    "    # format when looking for songs.\n",
    "    index_artist_name = artist_name.lower().replace(' ', '-')\n",
    "    index_title = title.lower().replace(' ', '-')\n",
    "    if (index_artist_name, index_title) in lyrics_df2.index:\n",
    "        lyrics = lyrics_df2.loc[index_artist_name, index_title].values[0][0]\n",
    "        if len(lyrics) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return lyrics\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(artist_name, title):\n",
    "    lyrics = get_lyrics_csv1(artist_name, title)\n",
    "    if lyrics:\n",
    "        return lyrics\n",
    "    \n",
    "    lyrics = get_lyrics_csv2(artist_name, title)\n",
    "    if lyrics:\n",
    "        return lyrics\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a new dataframe that contains lyrics information for our previous data. If the lyrics are not found in either of the lyrics datasets, we generate the genius.com url to search for that song's lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match lyrics\n",
    "data_lyrics = data.copy()\n",
    "urls = {}\n",
    "i = 1\n",
    "for index, row in data.iterrows():\n",
    "    lyrics = get_lyrics(row['artist_name'], row['title'])\n",
    "    \n",
    "    if lyrics == \"\":\n",
    "        # To create the URL to find the song on genius, the title and artist names\n",
    "        # need to be processed to match the general format of Genius' songs URL.\n",
    "        # For instance, spaces are replaced by hyphens and additional information\n",
    "        # between parenthesis is removed.\n",
    "        url = (row['artist_name'].lower().replace(' ', '-') + '-' + re.sub(r'\\([^)]*\\)', '', row['title']).rstrip().lower().replace(' ', '-') + '-lyrics').capitalize().replace(\"'\", '')\n",
    "        urls[index] = 'https://genius.com/' + url\n",
    "        \n",
    "    print(i, end='\\r')\n",
    "    i += 1\n",
    "    data_lyrics.loc[index, 'lyrics'] = re.sub(r'[\\[].*?[\\]]', '', lyrics.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genius.com URLs are collected in a file so that they can be fed into a scrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/urls', 'w') as urls_files:\n",
    "    for index, url in urls.items():\n",
    "        print(index, url, file=urls_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we run our scrapper which is using `scrapy`. This is not done in this notebook but instead you can find the scrapper code in the `scrapper` folder in this repository. We obtain a file that contains the track ids as well as their lyrics found on genius.com.\n",
    "\n",
    "The resulting file is then read and its data is added to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/missing_lyrics.json') as lyrics_file:\n",
    "    lyrics_json = json.load(lyrics_file)\n",
    "    for item in lyrics_json:\n",
    "        for index, lyrics in item.items():\n",
    "            data_lyrics.loc[index, 'lyrics'] = re.sub(r'[\\[].*?[\\]]', '', lyrics.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then detect the language of the lyrics if any. It is possible that some of the lyrics do not contain any features that allow language detection, in this case we do not assign a language to that lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the corresponding language for the lyrics\n",
    "from langdetect import detect\n",
    "\n",
    "for index, row in data_lyrics.iterrows():\n",
    "    lyrics = data_lyrics.loc[index, 'lyrics']\n",
    "    language = None\n",
    "    if lyrics.strip() != \"\":\n",
    "        try:\n",
    "            language = detect(lyrics)\n",
    "        except:\n",
    "            language = \"\"\n",
    "    if language != \"\":\n",
    "        data_lyrics.loc[index, 'lang'] = language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how many songs are there with lyrics in the Million Song Subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics = data_lyrics[data_lyrics.lyrics != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the resulting data in a file for convenience. This is the final state of our data and contains verything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics.to_csv('data/data_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain our analysis, we will follow the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to be able to extract different themes from our lyrics. Other than being able to see the evolution of some words over time and depending of the genres of the song, it's interesting to see the themes or sentiments that the song's lyrics portray. For this, we will use Natural Language Processing (NLP) libraries to extract this information about each track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lyrics that we have, we apply the bag-of-words model and only keep the interesting (meaningful) words. That is, we remove the stop words and lemmatize each word to avoid, for instance, having both 'sleep' and 'sleeping'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading `nltk` packages that will be of use for us. [ntlk](http://www.nltk.org/) is a well known framework for natural language processing in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define a few functions to do all our natural language analysis steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function that tokenizes lyrics. As we have seen in class, working on a list of tokens instead of a string of characters is much better for machine learning and natural language processing techniques that we will use. For this we use TextBlob which we will also use later on for sentiment analysis since it gives a nice way to obtain tagged tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return TextBlob(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a function that removes stopwords from our tokens. This function uses standard stop words list from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_full_name(isocode):\n",
    "    return pycountry.languages.get(alpha_2=isocode).name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_languages = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(blob, language):\n",
    "    if language not in stop_words_languages:\n",
    "        stop_words_languages[language] = set(stop_words.words(get_language_full_name(language)))\n",
    "        \n",
    "    tokens = []\n",
    "    for word, tag in blob.tags:\n",
    "        lower = word.lower().replace(\"'\", '')\n",
    "        if lower not in stop_words_languages[language]:\n",
    "            tokens.append((lower, tag))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next function, we lemmatize the tokens that are left so that words that variants of words that are essentially the same (conjugated verbs for examples) are counted as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmas = []\n",
    "    lemma = None\n",
    "    for token, tag in tokens:\n",
    "        if tag[0] == \"V\": #if the word is a verb\n",
    "            lemma = Word(token).lemmatize(\"v\")\n",
    "        else:\n",
    "            lemma = Word(token).lemmatize()\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function combines the functions defined above to get the final tokens that we will consider for our topic detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_tokens(lyrics):\n",
    "    texts = []\n",
    "    for lyric in lyrics:\n",
    "        texts.append(lemmatize(remove_stopwords(tokenize(lyric), 'en')))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data visualisation, we also need to have raw frequencies of the words that we use. While later on we will use different ways to obtain such information for our ML techniques, here for the visualisation we want a quick and simple count of the appearence of our tokens over the whole corpus passed as parameters. This way we can easily obtain the frequency of words appearing in all the lyrics for a given genre in a given year for example. The function defined below does precisely that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(texts):\n",
    "    word_count = {}\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            if token not in word_count:\n",
    "                word_count[token] = 1\n",
    "            else:\n",
    "                word_count[token] += 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the general sentiment of a song's lyrics, we use [TextBlob](https://textblob.readthedocs.io/en/dev/). This library has already a built-in sentiment analyser, which gives us inforation whether the song is 'positive' or 'negative'. The following function is used to do just that and takes as parameter a single lyrics string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(lyrics):\n",
    "    blob = TextBlob(lyrics)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything needed defined, we begin by getting our data (from the file written to in the previous section) into a dataframe. We only keep the lyrics that we detected to be in english since all our NLP and ML techniques wouldn't work on lyrics from different languages.\n",
    "\n",
    "We construct a genres list that will be used later in our data visualisation as well as a dictionary that links the genres to a list of indices of particular songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv('data/data_lyrics.csv')\n",
    "lyrics_df.set_index(['track_id'], inplace=True)\n",
    "\n",
    "lyrics_df = lyrics_df[lyrics_df.lang == 'en']\n",
    "genres_list = []\n",
    "genres_indices = {}\n",
    "for index, row in lyrics_df.iterrows():\n",
    "    genres = row['genres'].split('&')\n",
    "    for genre in genres:\n",
    "        genres_indices.setdefault(genre, []).append(index)\n",
    "        if genre not in genres_list:\n",
    "            genres_list.append(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our functions on the lyrics of each song and insert these tokens as a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['tokens'] = get_final_tokens(lyrics_df.lyrics.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add yet another column that is the sentiment of the lyrics of a particular song, obtained using the method described above. We scale the sentiment between 0 and 1 for our final visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = lyrics_df.lyrics.values\n",
    "sentiments = []\n",
    "for lyric in all_lyrics:\n",
    "    sentiment = get_sentiment(lyric)\n",
    "    sentiment = (sentiment + 1) / 2.0\n",
    "    sentiments.append(sentiment)\n",
    "lyrics_df['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect all our remaining data and output it into files that we will use for our visualisation. We first begin by aggregating the word frequencies by genre and year of release (songs without a year of release are dropped). Each word results in a file where each datapoint corresponds to a year and the frequency of the word in the particular genre and the particular year.\n",
    "\n",
    "The sentiment files are done in the same way, except that there is one file per genre.\n",
    "\n",
    "Moreover, a list of all words appearing for every genre is also created for our final data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_words = {}\n",
    "for genre, indices in genres_indices.items():\n",
    "    curr_df = lyrics_df.loc[indices]\n",
    "    freqs = get_word_freq(curr_df.tokens.values)\n",
    "    genres_words[genre] = list(freqs.keys())\n",
    "    word_count = sum(freqs.values())\n",
    "    \n",
    "    directory = 'data/final_data/words/' + genre\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    directory2 = 'data/final_data/sentiments'\n",
    "    if not os.path.exists(directory2):\n",
    "        os.makedirs(directory2)\n",
    "    \n",
    "    for year in curr_df[curr_df.year != 0].year.sort_values().unique():\n",
    "        curr_year_df = curr_df[curr_df.year == year]\n",
    "        freqs = get_word_freq(curr_year_df.tokens.values)\n",
    "        word_count = sum(freqs.values())\n",
    "        \n",
    "        sentiments = curr_year_df.sentiment.values\n",
    "        sentiment_avg = sum(sentiments) / float(len(sentiments))\n",
    "        filepath = 'data/final_data/sentiments/' + genre + '.csv'\n",
    "        if not os.path.exists(filepath):\n",
    "            with open(filepath, 'a') as output_file:\n",
    "                print('year,value', file=output_file)\n",
    "        with open(filepath, 'a') as output_file:\n",
    "            print(str(year) + ',' + str(sentiment_avg), file=output_file)\n",
    "            \n",
    "        for word, freq in freqs.items():\n",
    "            # remove undesirable characters\n",
    "            for ch in ['/', '*', '\"', ':', '\\\\']:\n",
    "                word = word.replace(ch, '-')\n",
    "            filepath = directory + '/' + word + '.csv'\n",
    "            if not os.path.exists(filepath):\n",
    "                with open(filepath, 'a') as output_file:\n",
    "                    print('year,value', file=output_file)\n",
    "            with open(filepath, 'a') as output_file:\n",
    "                print(str(year) + ',' + str(freq/float(word_count)), file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, the list of genres that we created earlier will be of use for our data visualisation as an auto-complete feature when searching for genres. The same is the case for the lists of words per genre. This information is saved in files as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre, words in genres_words.items():\n",
    "    with open('data/final_data/words/' + genre + '/allWords.json', 'w') as output_file:\n",
    "        json.dump(words, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final_data/allGenres.json', 'w') as output_file:\n",
    "    json.dump(genres_list, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, to find the topics of our songs, we use an LDA model, which is implemented in [Gensim](https://radimrehurek.com/gensim/). We will first train our model with a subset of the songs, and then use this model to infer the topics in all our lyrics.\n",
    "\n",
    "This method doesn't create a list of themes with clear names, but rather assumes that a topic is a collection of words, thus we will need to manually assign for each topic a name.\n",
    "\n",
    "The LDA model will output, for each song, a list of topics with different weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we will want to see our tracks grouped by the genres and the year of their release. This way we can compare the words used as well as themes portrayed in the tracks depending on their genres and their evolution over time. For this we aggregate the data on genre and year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualization, we would like to view our data in an insightful way. Our plan is to be able to enter a word and obtain a graph that shows the songs that contains this word. The songs would be identifiable by their genre and organised so that we can view the year of their release. The same would be possible for themes. This way we could explore interesting ideas about lyrical content of the songs over the past and obtain meaningful insights about them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
